---
layout: post
title: "pca"
description: ""
category: 
tags: []
draft: true
---
{% include JB/setup %}

PCA是principal component analysis的缩写，即主成分分析，此方法目标是找到数据中最主要的元素和结构，去除噪音和冗余，将原有的复杂数据降维，揭露出隐藏在复杂数据背后的简单结构。从线性袋鼠角度来看，PCA目标是找到一组正交基去重新描述得到的数据空间，这个维度就是主元。
混乱的数据中通常包含三种成分：噪音、旋转和冗余。在区分噪音的时候，可以使用信噪比或者方差来衡量，方差大的是主要信号或者主要分量；方差较小的则认为是噪音或者次要分量；对于旋转，则对基向量进行旋转，使得信噪比或者方差较大的基向量就是主元方向；在判断各个观测变量之间是否冗余时，可以借助协方差矩阵来进行衡量和判断。

PCA的主要思想：
1.最小化冗余量，对应于协方差矩阵的非对角线元素要尽量小；
2.最大化信号，对应于要使协方差矩阵的对角线上的元素尽可能的大。对角线上的元素值越大，也就是对应于越重要的主元。该思想实现过层就是对协方差矩阵进行对角化。

PCA的模型中存在很多的假设条件：
1.PCA的内部模型是线性的，kernel-PCA就是使用非线性的权值对PCA扩展；
2.针对的样本的概率分布模型只限于指数概率分布模型。从而扩展出ICP（独立主元分析）。
3.数据本身具有较高的信躁比。
4.假设主元向量之间是正交的。

为什么在PCA中，协方差矩阵的特征向量就是主元，等价于原矩阵的奇异值分解。
[奇异值分解及其应用](http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html)
[百度文库](http://wenku.baidu.com/view/14ca1548e518964bcf847c28.html?re=view)
使用svd解决PCA问题,可以得到两个方向上的PCA。
svd与LSI

PCA与LDA是特征抽取的两种主要经典方法
LDA(线性评判分析)
信号表示：特征抽取后的特征要能够精确地表示样本信息，使得信息丢失很小，对应的方法是PCA
信号分类：特征抽取后的特征，要使得分类后的准确率很高，不能比原来特征进行分类的准确率低。对于线性来说，对应的方法是LDA


LDA与PCA的目标不一样，导致他们的方法也不一样。PCA得到的投影空间是协方差矩阵的特征向量，而LDA则是通过求得一个变换W，使得变换之后的新均值之差最大，方差最大，变换W就是特征的投影方向。



